models:
#  - title: dolphin
#    provider: ollama
#    model: dolphin3-24b
#  - title: lms_llama
#    provider: lmstudio
#    model: qwen2.5-coder-32b-instruct-mlx
#    systemMessage: You are a helpful assistant that uses tools when appropriate.
#    default: true
#  - title: ollama_llama
#    provider: ollama
#    model: llama3.1
#  - model: my-model
#    title: custom-url-openai-compatible
#    apiBase: http://whatever:8080/v1
#    provider: openai
#  - title: dolphin-r1
#    provider: ollama
#    model: dolphin3-r1
#    temperature: 0.7
#    top_k: 40
#  - model: claude-3-7-sonnet-latest
#    provider: anthropic
#    apiKey: "****"
#    title: claude
#    temperature: 0.7
#    top_k: 256
#    top_p: 0.9
#    max_tokens: 2048
#  - model: gpt-4.1
#    title: gpt-4.1
#    provider: openai
  - model: o4-mini
    title: o4-mini
    provider: openai
    is_reasoning: True
    reasoning_effort: "medium"
#  - model: o3-mini
#    title: o3-mini
#    systemMessage: You are an expert software developer. You give helpful and concise responses.
#    contextLength: 128000
#    maxCompletionTokens: 65536
#    apiKey: "****"
#    provider: openai
#    temperature: 0.2
#    top_p: 0.8
#  - model: llama3.3:70b-instruct-q3_K_M
#    title: llama3.3:70b
#    client: http://mammoth:11434
#    keep_alive_seconds: '240'
#    provider: lmstudio
#  - model: qwen2.5:72b-instruct-q3_K_S
#    title: qwen2.5:72b
#    client: http://mammoth:11434
#    keep_alive_seconds: '240'
#    provider: lmstudio

tool_timeout: 120 # Tool timeout in seconds for all MCP servers. This is the maximum time a tool can run before being forcibly stopped.

mcpServers:
#  mcp-atlassian:
#    command: docker
#    args:
#      - run
#      - -i
#      - --rm
#      - -e
#      - CONFLUENCE_URL
#      - -e
#      - CONFLUENCE_USERNAME
#      - -e
#      - CONFLUENCE_API_TOKEN
#      - -e
#      - JIRA_URL
#      - -e
#      - JIRA_USERNAME
#      - -e
#      - JIRA_API_TOKEN
#      - ghcr.io/sooperset/mcp-atlassian:latest
#    env:
#      CONFLUENCE_URL: https://ekoapp.atlassian.net/wiki
#      CONFLUENCE_USERNAME: touchapon@amity.co
#      CONFLUENCE_API_TOKEN: "-"
#      JIRA_URL: https://ekoapp.atlassian.net
#      JIRA_USERNAME: touchapon@amity.co
#      JIRA_API_TOKEN: "-"
#  github:
#    command: docker
#    args:
#      - run
#      - -i
#      - --rm
#      - -e
#      - GITHUB_PERSONAL_ACCESS_TOKEN
#      - -e
#      - GITHUB_TOOLSETS
#      - ghcr.io/github/github-mcp-server:latest
#    env:
#      GITHUB_PERSONAL_ACCESS_TOKEN: "-"
#      GITHUB_TOOLSETS: repos,issues,pull_requests,code_security,users
  # Example of a test server configuration for timeout testing:
  # test-timeout-server:
  #   command: python # Replace with your actual test server command/script
  #   args:
  #     - -m
  #     - http.server # Example: python -m http.server (will run indefinitely)
  #     # Or a script that sleeps:
  #     # - my_long_running_tool_server.py
  #     # - --sleep_duration
  #     # - "30" # This server's tool would sleep for 30 seconds
  #   # tool_timeout: 2 # REMOVED - Now global. Configure with the top-level 'tool_timeout'.
  #   # Ensure your test server script actually implements a tool that can be called.
  #   # The command above (http.server) is just a placeholder and won't work as an MCP server.
#  dolphin-demo-database-sqlite:
#    command: uvx
#    args:
#      - mcp-server-sqlite
#      - --db-path
#      - "~/.dolphin/dolphin.db"
